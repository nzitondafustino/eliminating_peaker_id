{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01b6f4-9007-45b3-b855-4a909821748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install speechbrain\n",
    "# !pip install ruamel_yaml\n",
    "# !pip install --upgrade ruamel.yaml --ignore-installed ruamel.yaml\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html \n",
    "# self.conv = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "# kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding),\n",
    "# PixelShuffle(upscale_factor),\n",
    "# nn.InstanceNorm2d(num_features=out_channels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8bedf4-d86c-4eee-ac18-ccc0b32f1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechbrain as sb\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6cd9f-3ecf-42e4-a48e-114e1320ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", \n",
    "                                       savedir=\"pretrained_models/asr-crdnn-rnnlm-librispeech\",run_opts={\"device\":\"cuda\"},freeze_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa6ad5-7592-4656-8cc2-fd5358ef85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.hparams.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973b9a9-d5e7-41a4-9f3c-fd5c30b9e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LIBRISPEECH(\".\", url= 'dev-clean',download=True)\n",
    "train_dataset = LIBRISPEECH(\".\",download=True)\n",
    "# dataset1 = MNIST(\".\",download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174612b5-a317-472b-a821-119ec178e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    \"\"\"waveform, sample_rate, transcript, speaker_id\"\"\"\n",
    "    waveforms = [b[0].permute([1,0]) for b in batch]\n",
    "    waveforms = pad_sequence(waveforms,batch_first=True)\n",
    "    waveforms = waveforms.squeeze()\n",
    "    input_len = torch.FloatTensor([b[0].shape[1] for b in batch])\n",
    "    input_len /= torch.max(input_len)\n",
    "    sampling_rates = torch.FloatTensor([b[1] for b in batch])\n",
    "    transcript = [b[2] for b in batch]\n",
    "    speaker_id = torch.LongTensor([b[3] for b in batch])\n",
    "    \n",
    "    return waveforms,input_len,sampling_rates,transcript,speaker_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc0cac-e7bc-4288-9ef9-37a2a10de96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataLoader = DataLoader(train_dataset,batch_size=24,shuffle=True,collate_fn=collate)\n",
    "val_dataLoader = DataLoader(val_dataset,batch_size=24,shuffle=True,collate_fn=collate)\n",
    "# train_dataloader = DataLoader(dataset1,batch_size=5,shuffle=True,collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4e20c-59da-4295-ba1a-8c9c751a0247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f1c097f-109c-40fe-93d1-867c0423d421",
   "metadata": {},
   "source": [
    "# highwat network\n",
    "- adopted from https://github.com/kefirski/pytorch_Highway.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17baf8f3-66f0-452e-aba0-b924f14c4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self, size, num_layers, f):\n",
    "\n",
    "        super(Highway, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.nonlinear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(num_layers)])\n",
    "\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            :param x: tensor with shape of [batch_size, size]\n",
    "            :return: tensor with shape of [batch_size, size]\n",
    "            applies σ(x) ⨀ (f(G(x))) + (1 - σ(x)) ⨀ (Q(x)) transformation | G and Q is affine transformation,\n",
    "            f is non-linear transformation, σ(x) is affine transformation with sigmoid non-linearition\n",
    "            and ⨀ is element-wise multiplication\n",
    "            \"\"\"\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            gate = torch.sigmoid(self.gate[layer](x))\n",
    "\n",
    "            nonlinear = self.f(self.nonlinear[layer](x))\n",
    "            linear = self.linear[layer](x)\n",
    "\n",
    "            x = gate * nonlinear + (1 - gate) * linear\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb0e31-a7c9-4e3e-a02f-f909fa198229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecsReconstruction(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_features,num_blocks,hidden_size,out_feature=40):\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.num_blocks = num_blocks\n",
    "        self.out_feature = out_feature\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_features,hidden_size) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.highway = Highway(hidden_size,num_blocks,nn.ReLU())\n",
    "        self.linear2 = nn.Linear(hidden_size,out_feature)\n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.highway(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6013450-9fdf-4d0c-9bdc-70bb263375f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = SpecsReconstruction(2560,5,100,40).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07308c23-e28d-4d49-b14d-3c07dd2af9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.L1Loss()\n",
    "# optimizer = torch.optim.Adam(h0.parameters(),lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2)\n",
    "# epochs = 30\n",
    "path = \"h0_states.pth\"\n",
    "def load_model_parameter(path):\n",
    "    try:\n",
    "        print(\"Loading states\")\n",
    "        state = torch.load(path)\n",
    "        start_epoch = state[\"epoch\"]\n",
    "        train_losses = state[\"train_losses\"]\n",
    "        val_losses = state[\"val_losses\"]\n",
    "        model_dict = state[\"model_dict\"]\n",
    "        optimizer = state[\"optimizer\"]\n",
    "        scheduler = state[\"scheduler\"]\n",
    "        print(\"successifully loaded states\")\n",
    "        return start_epoch,train_losses,val_losses,model_dict,optimizer,scheduler\n",
    "    except:\n",
    "        print(\"failed to load states\")\n",
    "        return None\n",
    "\n",
    "def save_model_parameters(path,state_dict):\n",
    "    torch.save(state_dict,path)\n",
    "    print(\"states at {} epoch saved\".format(state_dict[\"epoch\"]))\n",
    "    \n",
    "states = load_model_parameter(path)\n",
    "if states is not None:\n",
    "    start_epoch,train_losses,val_losses,model_dict,optimizer_dict,scheduler_dict = states\n",
    "    h0.load_state_dict(model_dict)\n",
    "    optimizer.load_state_dict(optimizer_dict)\n",
    "    scheduler.load_state_dict(scheduler_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9778e-bc64-447d-b202-08a43efcc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "min_loss = np.Infinity\n",
    "\n",
    "for i in range(epochs):\n",
    "    t = []\n",
    "    l = []\n",
    "    h0.train()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(train_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "        block_0 = block_0.reshape(block_0.shape[0],block_0.shape[1],-1)\n",
    "        out = h0(block_0)\n",
    "        loss = criterion(targets,out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (j+1) % 100 == 0:\n",
    "            print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "        t.append(loss.item())\n",
    "    av_t = sum(t)/len(t)\n",
    "    print(\"epoch:{}/{},Train loss:{}\".format(i+1,epochs,av_t))\n",
    "    train_loss.append(av_t)\n",
    "    del waveform\n",
    "    del input_len\n",
    "    # validation loop\n",
    "    h0.eval()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(val_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            block_0 = block_0.reshape(block_0.shape[0],block_0.shape[1],-1)\n",
    "            out = h0(block_0)\n",
    "            loss = criterion(targets,out)\n",
    "            if (j+1) % 100 == 0:\n",
    "                print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "            l.append(loss.item())\n",
    "    av_l = sum(l)/len(l)\n",
    "    print(\"epoch:{}/{},Val loss:{}\".format(i+1,epochs,av_l))\n",
    "    val_loss.append(av_l)\n",
    "    if av_l < min_loss:\n",
    "        min_loss = av_l\n",
    "        torch.save(h0,\"best_model_0\")\n",
    "        \n",
    "    state_dict = {\n",
    "    \"epoch\":i,\n",
    "    \"train_losses\":train_loss,\n",
    "    \"val_losses\":val_loss,\n",
    "    \"model_dict\":h0.state_dict(),\n",
    "    \"optimizer\":optimizer.state_dict(),\n",
    "    \"scheduler\":scheduler.state_dict()\n",
    "    }\n",
    "    save_model_parameters(\"h0_states.pth\",state_dict)\n",
    "    scheduler.step(av_l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842290a-987f-4ce3-9f22-f2cb318a56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_1 = SpecsReconstruction(5120,5,100,40).cuda()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(h0_1.parameters(),lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2)\n",
    "epochs = 30\n",
    "stretch = torchaudio.transforms.TimeStretch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ab421-b56d-4acc-83bd-fbb55333a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"h0_1_states.pth\"\n",
    "def load_model_parameter(path):\n",
    "    try:\n",
    "        print(\"Loading states\")\n",
    "        state = torch.load(path)\n",
    "        start_epoch = state[\"epoch\"]\n",
    "        train_losses = state[\"train_losses\"]\n",
    "        val_losses = state[\"val_losses\"]\n",
    "        model_dict = state[\"model_dict\"]\n",
    "        optimizer = state[\"optimizer\"]\n",
    "        scheduler = state[\"scheduler\"]\n",
    "        print(\"successifully loaded states\")\n",
    "        return start_epoch,train_losses,val_losses,model_dict,optimizer,scheduler\n",
    "    except:\n",
    "        print(\"failed to load states\")\n",
    "        return None\n",
    "\n",
    "def save_model_parameters(path,state_dict):\n",
    "    torch.save(state_dict,path)\n",
    "    print(\"states at {} epoch saved\".format(state_dict[\"epoch\"]))\n",
    "    \n",
    "states = load_model_parameter(path)\n",
    "if states is not None:\n",
    "    start_epoch,train_losses,val_losses,model_dict,optimizer_dict,scheduler_dict = states\n",
    "    h0_1.load_state_dict(model_dict)\n",
    "    optimizer.load_state_dict(optimizer_dict)\n",
    "    scheduler.load_state_dict(scheduler_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dade52-4616-4a92-8423-1819b6b9bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "min_loss = np.Infinity\n",
    "\n",
    "for i in range(start_epoch,epochs):\n",
    "    t = []\n",
    "    l = []\n",
    "    h0_1.train()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(train_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            conv_1 = encoder.model.CNN.block_0.conv_1(targets)\n",
    "            norm_1 = encoder.model.CNN.block_0.norm_1(conv_1)\n",
    "            norm_1 =  norm_1.reshape( norm_1.shape[0], norm_1.shape[1],-1)\n",
    "        out = h0_1(norm_1)\n",
    "        loss = criterion(targets,out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (j+1) % 100 == 0:\n",
    "            print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "        t.append(loss.item())\n",
    "    av_t = sum(t)/len(t)\n",
    "    print(\"epoch:{}/{},Train loss:{}\".format(i+1,epochs,av_t))\n",
    "    train_loss.append(av_t)\n",
    "    del waveform\n",
    "    del input_len\n",
    "    # validation loop\n",
    "    h0_1.eval()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(val_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            conv_1 = encoder.model.CNN.block_0.conv_1(targets)\n",
    "            norm_1 = encoder.model.CNN.block_0.norm_1(conv_1)\n",
    "            norm_1 =  norm_1.reshape( norm_1.shape[0], norm_1.shape[1],-1)\n",
    "            out = h0_1(norm_1)\n",
    "            loss = criterion(targets,out)\n",
    "            if (j+1) % 100 == 0:\n",
    "                print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "            l.append(loss.item())\n",
    "    av_l = sum(l)/len(l)\n",
    "    print(\"epoch:{}/{},Val loss:{}\".format(i+1,epochs,av_l))\n",
    "    val_loss.append(av_l)\n",
    "    if av_l < min_loss:\n",
    "        min_loss = av_l\n",
    "        torch.save(h0_1,\"best_model_01\")\n",
    "        \n",
    "    state_dict = {\n",
    "    \"epoch\":i,\n",
    "    \"train_losses\":train_loss,\n",
    "    \"val_losses\":val_loss,\n",
    "    \"model_dict\":h0_1.state_dict(),\n",
    "    \"optimizer\":optimizer.state_dict(),\n",
    "    \"scheduler\":scheduler.state_dict()\n",
    "    }\n",
    "    save_model_parameters(\"h0_1_states.pth\",state_dict)\n",
    "    scheduler.step(av_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71068177-49a6-4987-85ce-43779fa56c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del h0\n",
    "h1 = SpecsReconstruction(2560,5,100,40).cuda()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(h1.parameters(),lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2)\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f34396-de68-4106-8f99-0acd6fa4541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "min_loss = np.Infinity\n",
    "\n",
    "for i in range(epochs):\n",
    "    t = []\n",
    "    l = []\n",
    "    h1.train()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(train_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            block_1 = encoder.model.CNN.block_1( block_0)\n",
    "            block_1 = block_1.reshape(block_1.shape[0],block_1.shape[1],-1)\n",
    "        out = h1(block_1)\n",
    "        loss = criterion(targets,out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (j+1) % 100 == 0:inputs\n",
    "            print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "        t.append(loss.item())\n",
    "    av_t = sum(t)/len(t)\n",
    "    print(\"epoch:{}/{},Train loss:{}\".format(i+1,epochs,av_t))\n",
    "    train_loss.append(av_t)\n",
    "    del waveform\n",
    "    del input_len\n",
    "    # validation loop\n",
    "    h1.eval()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(val_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            block_1 = encoder.model.CNN.block_1(block_0)\n",
    "            block_1 = block_1.reshape(block_1.shape[0],block_1.shape[1],-1)\n",
    "            out = h1(block_1)\n",
    "            loss = criterion(targets,out)\n",
    "            if (j+1) % 100 == 0:\n",
    "                print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "            l.append(loss.item())\n",
    "    av_l = sum(l)/len(l)\n",
    "    print(\"epoch:{}/{},Val loss:{}\".format(i+1,epochs,av_l))\n",
    "    val_loss.append(av_l)\n",
    "    if av_l < min_loss:\n",
    "        min_loss = av_l\n",
    "        torch.save(h1,\"best_model_1\")\n",
    "        \n",
    "    state_dict = {\n",
    "    \"epoch\":i,\n",
    "    \"train_losses\":train_loss,\n",
    "    \"val_losses\":val_loss,\n",
    "    \"model_dict\":h1.state_dict(),\n",
    "    \"optimizer\":optimizer.state_dict(),\n",
    "    \"scheduler\":scheduler.state_dict()\n",
    "    }\n",
    "    save_model_parameters(\"h1_states.pth\",state_dict)\n",
    "    scheduler.step(av_l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395248c0-c873-40f0-be36-92648abc441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_1 = SpecsReconstruction(5120,5,100,40).cuda()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(h1_1.parameters(),lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2)\n",
    "epochs = 30\n",
    "stretch = torchaudio.transforms.TimeStretch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f158d2-2bec-4ad6-9531-97502f14d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"h1_1_states.pth\"\n",
    "def load_model_parameter(path):\n",
    "    try:\n",
    "        print(\"Loading states\")\n",
    "        state = torch.load(path)\n",
    "        start_epoch = state[\"epoch\"]\n",
    "        train_losses = state[\"train_losses\"]\n",
    "        val_losses = state[\"val_losses\"]\n",
    "        model_dict = state[\"model_dict\"]\n",
    "        optimizer = state[\"optimizer\"]\n",
    "        scheduler = state[\"scheduler\"]\n",
    "        print(\"successifully loaded states\")\n",
    "        return start_epoch,train_losses,val_losses,model_dict,optimizer,scheduler\n",
    "    except:\n",
    "        print(\"failed to load states\")\n",
    "        return None\n",
    "\n",
    "def save_model_parameters(path,state_dict):\n",
    "    torch.save(state_dict,path)\n",
    "    print(\"states at {} epoch saved\".format(state_dict[\"epoch\"]))\n",
    "    \n",
    "states = load_model_parameter(path)\n",
    "if states is not None:\n",
    "    start_epoch,train_losses,val_losses,model_dict,optimizer_dict,scheduler_dict = states\n",
    "    h1_1.load_state_dict(model_dict)\n",
    "    optimizer.load_state_dict(optimizer_dict)\n",
    "    scheduler.load_state_dict(scheduler_dict)\n",
    "else:\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87401c13-6968-4713-888b-34673c1747d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "min_loss = np.Infinity\n",
    "\n",
    "for i in range(start_epoch,epochs):\n",
    "    t = []\n",
    "    l = []\n",
    "    h1_1.train()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(train_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            conv_1 = encoder.model.CNN.block_1.conv_1(block_0)\n",
    "            norm_1 = encoder.model.CNN.block_1.norm_1(conv_1)\n",
    "            norm_1 =  norm_1.reshape( norm_1.shape[0], norm_1.shape[1],-1)\n",
    "        out = h1_1(norm_1)\n",
    "        loss = criterion(targets,out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (j+1) % 100 == 0:\n",
    "            print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "        t.append(loss.item())\n",
    "    av_t = sum(t)/len(t)\n",
    "    print(\"epoch:{}/{},Train loss:{}\".format(i+1,epochs,av_t))\n",
    "    train_loss.append(av_t)\n",
    "    del waveform\n",
    "    del input_len\n",
    "    # validation loop\n",
    "    h1_1.eval()\n",
    "    for j,(waveform, input_len ,sample_rate, transcript, speaker_id) in enumerate(val_dataLoader,start=1):\n",
    "        waveform = waveform.cuda()\n",
    "        input_len = input_len.cuda()\n",
    "        with torch.no_grad():\n",
    "            specs = encoder.compute_features(waveform)\n",
    "            targets = encoder.normalize(specs,input_len)\n",
    "            block_0 = encoder.model.CNN.block_0(targets)\n",
    "            conv_1 = encoder.model.CNN.block_1.conv_1(block_0)\n",
    "            norm_1 = encoder.model.CNN.block_1.norm_1(conv_1)\n",
    "            norm_1 =  norm_1.reshape( norm_1.shape[0], norm_1.shape[1],-1)\n",
    "            out = h1_1(norm_1)\n",
    "            loss = criterion(targets,out)\n",
    "            if (j+1) % 100 == 0:\n",
    "                print(\"epoch:{}/{}\".format(i+1,epochs,j))\n",
    "            l.append(loss.item())\n",
    "    av_l = sum(l)/len(l)\n",
    "    print(\"epoch:{}/{},Val loss:{}\".format(i+1,epochs,av_l))\n",
    "    val_loss.append(av_l)\n",
    "    if av_l < min_loss:\n",
    "        min_loss = av_l\n",
    "        torch.save(h1_1,\"best_model_11\")\n",
    "        \n",
    "    state_dict = {\n",
    "    \"epoch\":i,\n",
    "    \"train_losses\":train_loss,\n",
    "    \"val_losses\":val_loss,\n",
    "    \"model_dict\":h1_1.state_dict(),\n",
    "    \"optimizer\":optimizer.state_dict(),\n",
    "    \"scheduler\":scheduler.state_dict()\n",
    "    }\n",
    "    save_model_parameters(\"h1_1_states.pth\",state_dict)\n",
    "    scheduler.step(av_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
